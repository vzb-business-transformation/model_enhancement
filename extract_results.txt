# Example of parsing a results text file
import re

def parse_results_file(file_path):
    with open(file_path, 'r') as f:
        content = f.read()

    results = {}
    models = ['rf', 'gb', 'xgb', 'lgb', 'cat', 'stacked_ensemble', 'voting_ensemble']

    for model in models:
        # Try to find the model section
        pattern = f"{model}:.*?R2: ([0-9.]+).*?MAE: ([0-9.]+).*?MSE: ([0-9.]+).*?RMSE: ([0-9.]+)"
        match = re.search(pattern, content, re.DOTALL)

        if match:
            results[model] = {
                'R2': float(match.group(1)),
                'MAE': float(match.group(2)),
                'MSE': float(match.group(3)),
                'RMSE': float(match.group(4))
            }

    return results



import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

def plot_model_metrics_comparison(results):
    """
    Create a comprehensive comparison of multiple metrics across different models.

    Args:
        results: Dictionary with model names as keys and performance metrics as values

    Returns:
        matplotlib Figure
    """
    # Extract metrics and model names
    model_names = list(results.keys())
    metrics = ['R2', 'MAE', 'RMSE']  # Focus on key metrics

    # Create DataFrame for plotting
    data = []
    for model_name, model_metrics in results.items():
        for metric in metrics:
            data.append({
                'Model': model_name,
                'Metric': metric,
                'Value': model_metrics[metric]
            })

    df_plot = pd.DataFrame(data)

    # Sort models by R2 score for consistent ordering
    r2_values = {model: metrics['R2'] for model, metrics in results.items()}
    sorted_models = sorted(r2_values.keys(), key=lambda x: r2_values[x], reverse=True)

    # Create figure with subplots
    fig, axes = plt.subplots(1, 3, figsize=(18, 8))

    # Color palette
    colors = sns.color_palette('viridis', len(model_names))
    model_colors = {model: color for model, color in zip(sorted_models, colors)}

    # Plot each metric
    for i, metric in enumerate(metrics):
        metric_data = df_plot[df_plot['Metric'] == metric]

        # Sort by model order
        metric_data['Model'] = pd.Categorical(
            metric_data['Model'],
            categories=sorted_models,
            ordered=True
        )
        metric_data = metric_data.sort_values('Model')

        # Define bar colors
        bar_colors = [model_colors[model] for model in metric_data['Model']]

        # Plot
        bars = axes[i].bar(
            metric_data['Model'],
            metric_data['Value'],
            color=bar_colors,
            alpha=0.7,
            edgecolor='black'
        )

        # Add value labels
        for bar in bars:
            height = bar.get_height()
            axes[i].text(
                bar.get_x() + bar.get_width()/2.,
                height,
                f'{height:.4f}',
                ha='center',
                va='bottom',
                rotation=90,
                fontsize=9
            )

        # Set title and labels
        axes[i].set_title(f'{metric} Comparison', fontsize=14)
        axes[i].set_ylabel('Score' if metric == 'R2' else 'Error', fontsize=12)

        # Rotate x-axis labels
        axes[i].set_xticklabels(metric_data['Model'], rotation=45, ha='right', fontsize=10)

        # Add grid
        axes[i].grid(axis='y', linestyle='--', alpha=0.3)

        # For R2, higher is better, so invert colormap
        if metric == 'R2':
            # Add reference line for perfect prediction
            axes[i].axhline(y=1.0, color='red', linestyle='--', alpha=0.5)
            axes[i].set_ylim(min(0.8, min(metric_data['Value']) - 0.05), 1.02)
        else:
            # For error metrics, lower is better
            axes[i].set_ylim(0, max(metric_data['Value']) * 1.1)

    # Add a common title
    plt.suptitle('Model Performance Metrics Comparison', fontsize=16, y=0.98)

    # Add interpretation guide
    interpretation_text = (
        "RÂ²: Higher is better (closer to 1.0)\n"
        "MAE: Lower is better (Mean Absolute Error)\n"
        "RMSE: Lower is better (Root Mean Squared Error)"
    )

    fig.text(0.5, 0.01, interpretation_text, ha='center', fontsize=10,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.7))

    # Tight layout
    plt.tight_layout(rect=[0, 0.05, 1, 0.95])

    return fig

# Example usage:
# results = {
#     'rf': {'R2': 0.8943, 'MAE': 0.5886, 'MSE': 2.7910, 'RMSE': 1.6706},
#     'gb': {'R2': 0.9090, 'MAE': 0.6628, 'MSE': 2.4028, 'RMSE': 1.5501},
#     'xgb': {'R2': 0.9116, 'MAE': 0.5668, 'MSE': 2.3342, 'RMSE': 1.5278},
#     'lgb': {'R2': 0.8925, 'MAE': 0.7595, 'MSE': 2.8384, 'RMSE': 1.6847},
#     'cat': {'R2': 0.9045, 'MAE': 0.6698, 'MSE': 2.5195, 'RMSE': 1.5873},
#     'stacked_ensemble': {'R2': 0.9163, 'MAE': 0.5869, 'MSE': 2.2098, 'RMSE': 1.4865},
#     'voting_ensemble': {'R2': 0.9116, 'MAE': 0.6015, 'MSE': 2.3343, 'RMSE': 1.5278}
# }
# fig = plot_model_metrics_comparison(results)
# plt.savefig('model_metrics_comparison.png', dpi=300, bbox_inches='tight')
# plt.show()