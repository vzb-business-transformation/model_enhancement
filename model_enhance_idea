import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from typing import Dict, List, Tuple, Any
import joblib
import os
import time
from datetime import datetime

class ModelEnhancer:
    """
    Class to enhance model performance by analyzing errors and incorporating feedback.
    """
    
    def __init__(self, 
                 data_processor=None, 
                 ml_models=None, 
                 original_model_path=None,
                 output_dir="results/enhanced",
                 random_state=42):
        """
        Initialize the model enhancer.
        
        Args:
            data_processor: DataProcessor instance
            ml_models: MLModels instance
            original_model_path: Path to original model
            output_dir: Directory for enhanced model outputs
            random_state: Random seed for reproducibility
        """
        self.data_processor = data_processor
        self.ml_models = ml_models
        self.original_model_path = original_model_path
        self.output_dir = output_dir
        self.random_state = random_state
        self.error_patterns = None
        self.feature_importance_shift = None
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
    def analyze_errors(self, 
                       jan_predictions: pd.DataFrame, 
                       jan_actuals: pd.Series,
                       features_df: pd.DataFrame,
                       threshold: float = 0.3) -> Dict:
        """
        Analyze prediction errors to identify patterns.
        
        Args:
            jan_predictions: DataFrame with model predictions
            jan_actuals: Series with actual values
            features_df: DataFrame with feature values
            threshold: Error threshold for analysis (as percentage)
            
        Returns:
            Dict of error patterns and insights
        """
        print("\n=== Analyzing Prediction Errors ===\n")
        
        # Create a DataFrame with predictions and actuals
        results = pd.DataFrame({
            'actual': jan_actuals
        })
        
        # Add predictions from each model
        for col in jan_predictions.columns:
            if col.startswith('prediction_'):
                model_name = col.replace('prediction_', '')
                results[f'pred_{model_name}'] = jan_predictions[col]
                
                # Calculate errors
                results[f'error_{model_name}'] = results[f'pred_{model_name}'] - results['actual']
                results[f'abs_error_{model_name}'] = results[f'error_{model_name}'].abs()
                results[f'pct_error_{model_name}'] = (results[f'abs_error_{model_name}'] / results['actual'].replace(0, 1)) * 100
        
        # Find high-error cases for each model
        error_insights = {}
        
        for model_name in [col.replace('pred_', '') for col in results.columns if col.startswith('pred_')]:
            # Identify high error cases
            high_errors = results[results[f'pct_error_{model_name}'] > threshold * 100].copy()
            
            print(f"Model {model_name}: {len(high_errors)} high-error cases out of {len(results)} ({len(high_errors)/len(results)*100:.2f}%)")
            
            if len(high_errors) == 0:
                error_insights[model_name] = {
                    'high_error_count': 0,
                    'error_bias': 0,
                    'correlated_features': {},
                    'error_by_value_range': {}
                }
                continue
                
            # Check for systematic bias in errors
            error_bias = results[f'error_{model_name}'].mean()
            over_predictions = (results[f'error_{model_name}'] > 0).sum()
            under_predictions = (results[f'error_{model_name}'] < 0).sum()
            
            print(f"  - Bias direction: {'Overprediction' if error_bias > 0 else 'Underprediction'}")
            print(f"  - Bias magnitude: {abs(error_bias):.2f}")
            print(f"  - Over-predictions: {over_predictions} ({over_predictions/len(results)*100:.2f}%)")
            print(f"  - Under-predictions: {under_predictions} ({under_predictions/len(results)*100:.2f}%)")
            
            # Merge errors with features for analysis
            if features_df is not None and len(features_df) == len(results):
                high_error_features = pd.concat([high_errors, features_df.loc[high_errors.index]], axis=1)
                
                # Find features correlated with errors
                correlations = {}
                numeric_features = features_df.select_dtypes(include=['int64', 'float64']).columns
                
                for feature in numeric_features:
                    if feature in features_df.columns:
                        corr = np.corrcoef(
                            features_df[feature].values, 
                            results[f'abs_error_{model_name}'].values
                        )[0, 1]
                        
                        if not np.isnan(corr):
                            correlations[feature] = corr
                
                # Sort correlations
                sorted_correlations = {k: v for k, v in sorted(correlations.items(), 
                                                             key=lambda item: abs(item[1]), 
                                                             reverse=True)}
                
                # Get top correlated features
                top_features = dict(list(sorted_correlations.items())[:10])
                
                print(f"  - Top features correlated with error:")
                for feature, corr in top_features.items():
                    print(f"    {feature}: {corr:.4f}")
                
                # Analyze errors by value ranges for top correlated features
                error_by_range = {}
                
                for feature, corr in list(top_features.items())[:3]:  # Analyze top 3 features
                    try:
                        # Create bins
                        bin_edges = np.quantile(features_df[feature], [0, 0.25, 0.5, 0.75, 1])
                        bin_labels = ['Q1', 'Q2', 'Q3', 'Q4']
                        
                        # Bin the data
                        features_df[f'{feature}_bin'] = pd.cut(
                            features_df[feature], 
                            bins=bin_edges, 
                            labels=bin_labels, 
                            include_lowest=True
                        )
                        
                        # Calculate mean error by bin
                        error_by_bin = results.groupby(features_df[f'{feature}_bin'])[f'abs_error_{model_name}'].mean()
                        error_by_range[feature] = error_by_bin.to_dict()
                        
                        print(f"  - Error by {feature} range:")
                        for bin_name, mean_error in error_by_bin.items():
                            print(f"    {bin_name}: {mean_error:.4f}")
                            
                    except Exception as e:
                        print(f"    Error analyzing {feature} by range: {str(e)}")
                
                # Store insights
                error_insights[model_name] = {
                    'high_error_count': len(high_errors),
                    'error_bias': error_bias,
                    'over_predictions': over_predictions,
                    'under_predictions': under_predictions,
                    'correlated_features': top_features,
                    'error_by_value_range': error_by_range
                }
            else:
                print("  - Cannot analyze feature correlation: features_df missing or size mismatch")
                error_insights[model_name] = {
                    'high_error_count': len(high_errors),
                    'error_bias': error_bias,
                    'over_predictions': over_predictions,
                    'under_predictions': under_predictions,
                    'correlated_features': {},
                    'error_by_value_range': {}
                }
            
            # Visualize error distribution
            plt.figure(figsize=(10, 6))
            sns.histplot(results[f'error_{model_name}'], kde=True)
            plt.title(f'Error Distribution for {model_name}')
            plt.xlabel('Prediction Error')
            plt.ylabel('Frequency')
            plt.axvline(x=0, color='r', linestyle='--')
            plt.savefig(os.path.join(self.output_dir, f'error_dist_{model_name}.png'))
            plt.close()
        
        self.error_patterns = error_insights
        return error_insights
    
    def add_targeted_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Add new features targeting specific error patterns.
        
        Args:
            X: Feature DataFrame
            
        Returns:
            DataFrame with additional features
        """
        print("\n=== Adding Targeted Features ===\n")
        
        if self.error_patterns is None:
            print("No error patterns available. Run analyze_errors first.")
            return X
        
        X_enhanced = X.copy()
        
        # Process each model's error patterns
        for model_name, insights in self.error_patterns.items():
            # Focus on top correlated features
            for feature, corr in insights['correlated_features'].items():
                if feature in X.columns and abs(corr) > 0.1:  # Only process features with meaningful correlation
                    print(f"Adding targeted features for {feature} (correlation: {corr:.4f})")
                    
                    # Create polynomial features
                    X_enhanced[f'{feature}_squared'] = X[feature] ** 2
                    X_enhanced[f'{feature}_cubed'] = X[feature] ** 3
                    
                    # Create log transformation
                    if (X[feature] > 0).all():
                        X_enhanced[f'log_{feature}'] = np.log1p(X[feature])
                    
                    # Create interaction features between top correlated features
                    for other_feature, other_corr in insights['correlated_features'].items():
                        if (other_feature in X.columns and 
                            feature != other_feature and 
                            abs(other_corr) > 0.1):
                            X_enhanced[f'{feature}_x_{other_feature}'] = X[feature] * X[other_feature]
            
            # Add correction factor for systematic bias
            if abs(insights['error_bias']) > 5:  # Only add if bias is significant
                bias_direction = 1 if insights['error_bias'] > 0 else -1
                
                # Calculate adjustment factor based on bias
                X_enhanced[f'bias_correction_{model_name}'] = bias_direction * abs(insights['error_bias']) * 0.5
                
                print(f"Adding bias correction factor for {model_name}: {insights['error_bias']:.4f}")
        
        # Check how many new features were added
        new_features = [col for col in X_enhanced.columns if col not in X.columns]
        print(f"Added {len(new_features)} new targeted features")
        
        return X_enhanced
    
    def retrain_with_feedback(self, 
                             train_data: pd.DataFrame, 
                             jan_data: pd.DataFrame, 
                             target_col: str = 'DISCO_DURATION') -> Dict:
        """
        Retrain models with January data incorporated.
        
        Args:
            train_data: Original training data
            jan_data: January data with actual values
            target_col: Target column name
            
        Returns:
            Dict of enhanced models
        """
        print("\n=== Retraining Models with Feedback ===\n")
        
        # Combine original training data with January data
        combined_data = pd.concat([train_data, jan_data], ignore_index=True)
        print(f"Combined data shape: {combined_data.shape}")
        
        # Preprocess combined data
        processed_df = self.data_processor.preprocess_data(combined_data, is_training=True, target_col=target_col)
        print(f"Processed data shape: {processed_df.shape}")
        
        # Prepare train/test split
        X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled = self.data_processor.prepare_train_test_data(
            processed_df, target_col=target_col, test_size=0.2
        )
        
        # Feature selection with original method
        X_train_selected = self.ml_models.select_features(X_train, y_train, threshold=0.001, method='rf')
        X_test_selected = X_test[X_train_selected.columns]
        
        # Add targeted features based on error analysis
        X_train_enhanced = self.add_targeted_features(X_train_selected)
        X_test_enhanced = self.add_targeted_features(X_test_selected)
        
        # Remove highly correlated features
        X_train_final = self.ml_models.drop_correlated_features(X_train_enhanced, threshold=0.95)
        X_test_final = X_test_enhanced[X_train_final.columns]
        
        print(f"Final enhanced feature set: {X_train_final.shape[1]} features")
        
        # Train models with new feature set
        models_to_train = ['rf', 'gb', 'xgb', 'lgb', 'cat']
        enhanced_models = self.ml_models.train_all_models(X_train_final, y_train, models_to_train=models_to_train)
        
        # Create ensembles
        if len(enhanced_models) >= 2:
            stacked_model, stacked_metrics = self.ml_models.create_stacked_ensemble(
                X_train_final, X_test_final, y_train, y_test
            )
            
            voting_model, voting_metrics = self.ml_models.create_voting_ensemble(
                X_train_final, X_test_final, y_train, y_test
            )
        
        # Evaluate all models
        all_results = self.ml_models.evaluate_models(X_test_final, y_test)
        
        # Find best model
        best_model_name = max(all_results.items(), key=lambda x: x[1]['R2'])[0]
        best_r2 = all_results[best_model_name]['R2']
        print(f"\nBest enhanced model: {best_model_name} with R² = {best_r2:.4f}")
        
        # Compare feature importance before and after enhancement
        if hasattr(self.ml_models, 'feature_importances') and 'rf' in self.ml_models.feature_importances:
            old_importances = pd.read_joblib(os.path.join(os.path.dirname(self.original_model_path), 
                                                      "feature_importances.joblib"))
            
            if 'rf' in old_importances:
                old_top_features = old_importances['rf'].head(20)
                new_top_features = self.ml_models.feature_importances['rf'].head(20)
                
                # Find changes in feature importance
                common_features = set(old_top_features.index) & set(new_top_features.index)
                
                importance_shifts = {}
                for feature in common_features:
                    old_rank = old_top_features.index.get_loc(feature)
                    new_rank = new_top_features.index.get_loc(feature)
                    rank_change = old_rank - new_rank
                    
                    old_importance = old_top_features[feature]
                    new_importance = new_top_features[feature]
                    importance_change = (new_importance - old_importance) / old_importance * 100
                    
                    importance_shifts[feature] = {
                        'old_rank': old_rank,
                        'new_rank': new_rank,
                        'rank_change': rank_change,
                        'importance_change_pct': importance_change
                    }
                
                self.feature_importance_shift = importance_shifts
                
                # Print top changes
                print("\nTop feature importance changes:")
                for feature, changes in sorted(importance_shifts.items(), 
                                           key=lambda x: abs(x[1]['importance_change_pct']), 
                                           reverse=True)[:5]:
                    print(f"  {feature}: {changes['importance_change_pct']:.2f}% change, rank: {changes['old_rank']} → {changes['new_rank']}")
        
        # Save enhanced models
        self.ml_models.save_models(os.path.join(self.output_dir, 'models'))
        
        # Save data processor
        self.data_processor.save_preprocessor(os.path.join(self.output_dir, 'models', 'data_processor.joblib'))
        
        return all_results
    
    def predict_future(self, feb_data: pd.DataFrame, target_col: str = 'DISCO_DURATION') -> pd.DataFrame:
        """
        Generate predictions for February data.
        
        Args:
            feb_data: February data for prediction
            target_col: Target column name
            
        Returns:
            DataFrame with predictions
        """
        print("\n=== Generating February Predictions ===\n")
        
        # Preprocess February data
        processed_df = self.data_processor.preprocess_data(feb_data, is_training=False, target_col=target_col)
        
        # Create prediction dataframe
        original_ids = feb_data['CIR_ID'] if 'CIR_ID' in feb_data.columns else pd.Series(range(len(feb_data)))
        results_df = pd.DataFrame({'CIR_ID': original_ids})
        
        # Add actual values if available
        if target_col in feb_data.columns:
            results_df['actual'] = feb_data[target_col]
        
        # Add targeted features to processed data
        enhanced_features = self.add_targeted_features(processed_df)
        
        # Make predictions with each model
        predictions = {}
        
        for name, model in self.ml_models.best_models.items():
            try:
                if target_col in enhanced_features.columns:
                    X = enhanced_features.drop(columns=[target_col])
                else:
                    X = enhanced_features
                
                preds = model.predict(X)
                predictions[name] = preds
                results_df[f'prediction_{name}'] = preds
            except Exception as e:
                print(f"Error predicting with model {name}: {str(e)}")
        
        # Create ensemble prediction (average of all models)
        if len(predictions) > 1:
            ensemble_preds = np.mean([predictions[name] for name in predictions], axis=0)
            results_df['prediction_ensemble'] = ensemble_preds
        
        # Save predictions
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(self.output_dir, f'feb_predictions_{timestamp}.csv')
        results_df.to_csv(output_file, index=False)
        
        print(f"February predictions saved to {output_file}")
        
        return results_df
    
    def save_enhancement_report(self, 
                               error_analysis: Dict, 
                               model_results: Dict,
                               start_time: float):
        """
        Save a detailed report of the enhancement process.
        
        Args:
            error_analysis: Results from error analysis
            model_results: Results from model evaluation
            start_time: Start time for timing
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(self.output_dir, f'enhancement_report_{timestamp}.txt')
        
        with open(report_file, 'w') as f:
            f.write("=== Circuit Prediction Model Enhancement Report ===\n")
            f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("=== Error Analysis Summary ===\n")
            for model_name, insights in error_analysis.items():
                f.write(f"\nModel: {model_name}\n")
                f.write("-" * (len(model_name) + 8) + "\n")
                f.write(f"High-error cases: {insights['high_error_count']}\n")
                f.write(f"Error bias: {insights['error_bias']:.4f} ({insights['over_predictions']} over, {insights['under_predictions']} under)\n")
                
                f.write("\nTop features correlated with error:\n")
                for feature, corr in insights['correlated_features'].items():
                    f.write(f"  {feature}: {corr:.4f}\n")
                
                f.write("\nError by feature range:\n")
                for feature, ranges in insights['error_by_value_range'].items():
                    f.write(f"  {feature}:\n")
                    for bin_name, mean_error in ranges.items():
                        f.write(f"    {bin_name}: {mean_error:.4f}\n")
            
            f.write("\n\n=== Enhanced Model Performance ===\n")
            for model_name, metrics in model_results.items():
                f.write(f"\n{model_name}:\n")
                f.write("-" * (len(model_name) + 1) + "\n")
                for metric_name, value in metrics.items():
                    f.write(f"{metric_name}: {value:.4f}\n")
            
            if self.feature_importance_shift:
                f.write("\n\n=== Feature Importance Shifts ===\n")
                for feature, changes in sorted(self.feature_importance_shift.items(), 
                                           key=lambda x: abs(x[1]['importance_change_pct']), 
                                           reverse=True)[:10]:
                    f.write(f"{feature}:\n")
                    f.write(f"  Importance change: {changes['importance_change_pct']:.2f}%\n")
                    f.write(f"  Rank change: {changes['rank_change']} (from {changes['old_rank']} to {changes['new_rank']})\n")
            
            f.write(f"\n\nEnhancement process completed in {time.time() - start_time:.2f} seconds\n")
        
        print(f"Enhancement report saved to {report_file}")







# enhance_model.py
import pandas as pd
import numpy as np
import os
import time
from datetime import datetime
import argparse

# Import your existing modules
from data_processing import DataProcessor
from ml_models import MLModels
from model_enhancer import ModelEnhancer

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Circuit Prediction Model Enhancement')
    
    parser.add_argument('--jan_predictions', type=str, required=True,
                       help='Path to January predictions file')
    
    parser.add_argument('--jan_actuals', type=str, required=True,
                       help='Path to January actuals data')
    
    parser.add_argument('--feb_data', type=str, required=True,
                       help='Path to February data for prediction')
    
    parser.add_argument('--original_model_path', type=str, default='results/models',
                       help='Path to original model directory')
    
    parser.add_argument('--output_dir', type=str, default='results/enhanced',
                       help='Directory for enhanced results')
    
    parser.add_argument('--target_col', type=str, default='DISCO_DURATION',
                       help='Target column name')
    
    parser.add_argument('--train_data', type=str, default=None,
                        help='Path to original training data (optional, for retraining)')
    
    parser.add_argument('--error_threshold', type=float, default=0.3,
                       help='Error threshold for analysis (as percentage)')
    
    parser.add_argument('--random_state', type=int, default=42,
                       help='Random seed for reproducibility')
    
    return parser.parse_args()

def load_data(file_path):
    """Load data from various file formats"""
    if file_path.endswith('.csv'):
        return pd.read_csv(file_path)
    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
        return pd.read_excel(file_path)
    elif file_path.endswith('.parquet'):
        return pd.read_parquet(file_path)
    else:
        raise ValueError(f"Unsupported file format: {file_path}")

def main():
    """Main function to enhance the prediction model"""
    print("\n=== Starting Circuit Prediction Model Enhancement ===\n")
    start_time = time.time()
    
    # Parse arguments
    args = parse_arguments()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load January predictions
    print(f"Loading January predictions from {args.jan_predictions}")
    jan_predictions = load_data(args.jan_predictions)
    
    # Load January actuals
    print(f"Loading January actuals from {args.jan_actuals}")
    jan_actuals = load_data(args.jan_actuals)
    
    # Load February data
    print(f"Loading February data from {args.feb_data}")
    feb_data = load_data(args.feb_data)
    
    # Initialize components
    data_processor = DataProcessor(random_state=args.random_state)
    ml_models = MLModels(random_state=args.random_state)
    
    # Load existing models and preprocessor
    data_processor.load_preprocessor(os.path.join(args.original_model_path, 'data_processor.joblib'))
    ml_models.load_models(args.original_model_path)
    
    # Initialize the model enhancer
    enhancer = ModelEnhancer(
        data_processor=data_processor,
        ml_models=ml_models,
        original_model_path=args.original_model_path,
        output_dir=args.output_dir,
        random_state=args.random_state
    )
    
    # Extract actual values from January data
    jan_actual_values = jan_actuals[args.target_col]
    
    # Get features used for predictions
    jan_features = data_processor.preprocess_data(jan_actuals, is_training=False, target_col=args.target_col)
    
    # Analyze prediction errors
    error_insights = enhancer.analyze_errors(
        jan_predictions=jan_predictions,
        jan_actuals=jan_actual_values,
        features_df=jan_features,
        threshold=args.error_threshold
    )
    
    # Load original training data if provided
    if args.train_data:
        print(f"Loading original training data from {args.train_data}")
        train_data = load_data(args.train_data)
        
        # Retrain with January feedback
        model_results = enhancer.retrain_with_feedback(
            train_data=train_data,
            jan_data=jan_actuals,
            target_col=args.target_col
        )
    else:
        # If no training data provided, use only error analysis for enhancement
        print("No training data provided. Skipping retraining.")
        model_results = ml_models.evaluate_models(
            jan_features.drop(columns=[args.target_col]) if args.target_col in jan_features.columns else jan_features,
            jan_actual_values
        )
    
    # Generate February predictions
    feb_predictions = enhancer.predict_future(
        feb_data=feb_data,
        target_col=args.target_col
    )
    
    # Save enhancement report
    enhancer.save_enhancement_report(
        error_analysis=error_insights,
        model_results=model_results,
        start_time=start_time
    )
    
    print(f"\nModel enhancement completed in {time.time() - start_time:.2f} seconds")
    print(f"Enhanced model and predictions saved to {args.output_dir}")













////////////////////////////////////////////////









# enhance_predict_sequence.py
import pandas as pd
import numpy as np
import os
import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from data_processing import DataProcessor
from ml_models import MLModels
from src.run_query import run_sql

def main():
    """Main function to run the sequential prediction and enhancement workflow"""
    print("\n=== Starting Sequential Circuit Prediction Enhancement Pipeline ===\n")
    start_time = time.time()
    
    # Set up directories
    base_output_dir = "results"
    jan_output_dir = os.path.join(base_output_dir, "january")
    enhanced_output_dir = os.path.join(base_output_dir, "enhanced")
    models_dir = os.path.join(base_output_dir, "models")
    
    os.makedirs(jan_output_dir, exist_ok=True)
    os.makedirs(enhanced_output_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # 1. Load data
    print("Loading training data (2023-2024)...")
    
    # Training data (completed disconnects from 2023-2024)
    train_query = """
    select distinct a.CIR_ID,a.NASP_ID, NASP_NM, DOM_INTL_FLAG, INT_EXT_FLAG, LVL_4_PRD_NM,CIR_INST_DATE,CIR_DISC_DATE,VRTCL_MRKT_NAME,
    SALES_TIER, SR_CRCTP, RPTG_CRCTP, RPTG_CIR_SPEED, ACCESS_SPEED, ACCESS_SPEED_UP, 
    PORT_SPEED, PVC_CAR_SPEED, ACCESS_DISC, PORT_DISC, PVC_CAR_DISC, OTHER_DISC, OTHER_ADJ,
    TOTAL_REV-(TOTAL_COST + OCC_COST_NONAFLT_M6364) MARGIN,
    REV_TYPE_FLAG, COST_TYPE_FLAG, FIRST_ADDR_TYPE_CODE,
    FIRST_CITY, FIRST_STATE, FIRST_ZIP,
    FIRST_ONNET, FIRST_INREGION, FIRST_XOLIT, FIRST_LATA,
    ONNET_PROV, COMPANY_CODE, VRD_FLAG, OPCO_REV, IN_FOOTPRINT, NASP_TYPE,
    CIR_TECH_TYPE, CIR_BILL_TYPE, PROGRAM, VENDOR,
    BIZ_CASE, FIRST_LAT, FIRST_LGNTD,MIG_STATUS,
    SECOND_LAT, SECOND_LGNTD, IEN_PROV, DIV_PORT, DIV_ACCESS, a.PROD_YR_MTH,
    DISCO_ORD_NUM,DISCO_DATE_BCOM,DISCO_DATE_ORDERING_STRT,DISCO_DATE_BCOM - DISCO_DATE_ORDERING_STRT as DISCO_DURATION
    from edw_sr_vw.rt_cir_single_row_addr a
    inner join (
        select conv_naspid NASP_ID,CIR_ID,INST_ORD_NUM,CHG_ORD_NUM,MIG_STATUS,DISCO_ORD_NUM,DISCO_DATE_BCOM,DISCO_DATE_ORDERING_STRT
        from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW
        where report_date < (select max(report_date) from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW)
        qualify row_number() over(partition by DISCO_ORD_NUM order by REPORT_DATE desc) = 1
    ) b ON a.CIR_ID = b.CIR_ID
    where PROD_YR_MTH >= 202301 and PROD_YR_MTH <= 202412
    and REV_LOC_DIV_CODE in ('LRG','PUB','WHL','SAM')
    and DISCO_DATE_BCOM is not null
    and DISCO_DATE_ORDERING_STRT is not null
    and DISCO_ORD_NUM <> ''
    and DISCO_DURATION > 0 
    and DISCO_DURATION <= 365  -- Filter unreasonable durations
    """
    
    # January 2025 data for evaluation (completed disconnects)
    jan_actual_query = """
    select distinct a.CIR_ID,a.NASP_ID, NASP_NM, DOM_INTL_FLAG, INT_EXT_FLAG, LVL_4_PRD_NM,CIR_INST_DATE,CIR_DISC_DATE,VRTCL_MRKT_NAME,
    SALES_TIER, SR_CRCTP, RPTG_CRCTP, RPTG_CIR_SPEED, ACCESS_SPEED, ACCESS_SPEED_UP, 
    PORT_SPEED, PVC_CAR_SPEED, ACCESS_DISC, PORT_DISC, PVC_CAR_DISC, OTHER_DISC, OTHER_ADJ,
    TOTAL_REV-(TOTAL_COST + OCC_COST_NONAFLT_M6364) MARGIN,
    REV_TYPE_FLAG, COST_TYPE_FLAG, FIRST_ADDR_TYPE_CODE,
    FIRST_CITY, FIRST_STATE, FIRST_ZIP,
    FIRST_ONNET, FIRST_INREGION, FIRST_XOLIT, FIRST_LATA,
    ONNET_PROV, COMPANY_CODE, VRD_FLAG, OPCO_REV, IN_FOOTPRINT, NASP_TYPE,
    CIR_TECH_TYPE, CIR_BILL_TYPE, PROGRAM, VENDOR,
    BIZ_CASE, FIRST_LAT, FIRST_LGNTD,MIG_STATUS,
    SECOND_LAT, SECOND_LGNTD, IEN_PROV, DIV_PORT, DIV_ACCESS, a.PROD_YR_MTH,
    DISCO_ORD_NUM,DISCO_DATE_BCOM,DISCO_DATE_ORDERING_STRT,DISCO_DATE_BCOM - DISCO_DATE_ORDERING_STRT as DISCO_DURATION
    from edw_sr_vw.rt_cir_single_row_addr a
    inner join (
        select conv_naspid NASP_ID,CIR_ID,INST_ORD_NUM,CHG_ORD_NUM,MIG_STATUS,DISCO_ORD_NUM,DISCO_DATE_BCOM,DISCO_DATE_ORDERING_STRT
        from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW
        where report_date < (select max(report_date) from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW)
        qualify row_number() over(partition by DISCO_ORD_NUM order by REPORT_DATE desc) = 1
    ) b ON a.CIR_ID = b.CIR_ID
    where PROD_YR_MTH = 202501
    and REV_LOC_DIV_CODE in ('LRG','PUB','WHL','SAM')
    and DISCO_DATE_BCOM is not null
    and DISCO_DATE_ORDERING_STRT is not null 
    and DISCO_ORD_NUM <> ''
    and DISCO_DURATION > 0
    and DISCO_DURATION <= 365
    """
    
    # January 2025 data to predict (in-progress disconnects)
    jan_predict_query = """
    select distinct a.CIR_ID,a.NASP_ID, NASP_NM, DOM_INTL_FLAG, INT_EXT_FLAG, LVL_4_PRD_NM,CIR_INST_DATE,CIR_DISC_DATE,VRTCL_MRKT_NAME,
    SALES_TIER, SR_CRCTP, RPTG_CRCTP, RPTG_CIR_SPEED, ACCESS_SPEED, ACCESS_SPEED_UP, 
    PORT_SPEED, PVC_CAR_SPEED, ACCESS_DISC, PORT_DISC, PVC_CAR_DISC, OTHER_DISC, OTHER_ADJ,
    TOTAL_REV-(TOTAL_COST + OCC_COST_NONAFLT_M6364) MARGIN,
    REV_TYPE_FLAG, COST_TYPE_FLAG, FIRST_ADDR_TYPE_CODE,
    FIRST_CITY, FIRST_STATE, FIRST_ZIP,
    FIRST_ONNET, FIRST_INREGION, FIRST_XOLIT, FIRST_LATA,
    ONNET_PROV, COMPANY_CODE, VRD_FLAG, OPCO_REV, IN_FOOTPRINT, NASP_TYPE,
    CIR_TECH_TYPE, CIR_BILL_TYPE, PROGRAM, VENDOR,
    BIZ_CASE, FIRST_LAT, FIRST_LGNTD,MIG_STATUS,
    SECOND_LAT, SECOND_LGNTD, IEN_PROV, DIV_PORT, DIV_ACCESS, a.PROD_YR_MTH,
    DISCO_ORD_NUM,DISCO_DATE_ORDERING_STRT
    from edw_sr_vw.rt_cir_single_row_addr a
    inner join (
        select conv_naspid NASP_ID,CIR_ID,INST_ORD_NUM,CHG_ORD_NUM,MIG_STATUS,DISCO_ORD_NUM,DISCO_DATE_ORDERING_STRT
        from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW
        where report_date < (select max(report_date) from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW)
        qualify row_number() over(partition by DISCO_ORD_NUM order by REPORT_DATE desc) = 1
    ) b ON a.CIR_ID = b.CIR_ID
    where PROD_YR_MTH = 202501
    and REV_LOC_DIV_CODE in ('LRG','PUB','WHL','SAM')
    and DISCO_DATE_BCOM is null  -- No completion date yet
    and DISCO_DATE_ORDERING_STRT is not null -- Has started disconnection
    and DISCO_ORD_NUM <> ''
    """
    
    # February 2025 data to predict (in-progress disconnects)
    feb_predict_query = """
    select distinct a.CIR_ID,a.NASP_ID, NASP_NM, DOM_INTL_FLAG, INT_EXT_FLAG, LVL_4_PRD_NM,CIR_INST_DATE,CIR_DISC_DATE,VRTCL_MRKT_NAME,
    SALES_TIER, SR_CRCTP, RPTG_CRCTP, RPTG_CIR_SPEED, ACCESS_SPEED, ACCESS_SPEED_UP, 
    PORT_SPEED, PVC_CAR_SPEED, ACCESS_DISC, PORT_DISC, PVC_CAR_DISC, OTHER_DISC, OTHER_ADJ,
    TOTAL_REV-(TOTAL_COST + OCC_COST_NONAFLT_M6364) MARGIN,
    REV_TYPE_FLAG, COST_TYPE_FLAG, FIRST_ADDR_TYPE_CODE,
    FIRST_CITY, FIRST_STATE, FIRST_ZIP,
    FIRST_ONNET, FIRST_INREGION, FIRST_XOLIT, FIRST_LATA,
    ONNET_PROV, COMPANY_CODE, VRD_FLAG, OPCO_REV, IN_FOOTPRINT, NASP_TYPE,
    CIR_TECH_TYPE, CIR_BILL_TYPE, PROGRAM, VENDOR,
    BIZ_CASE, FIRST_LAT, FIRST_LGNTD,MIG_STATUS,
    SECOND_LAT, SECOND_LGNTD, IEN_PROV, DIV_PORT, DIV_ACCESS, a.PROD_YR_MTH,
    DISCO_ORD_NUM,DISCO_DATE_ORDERING_STRT
    from edw_sr_vw.rt_cir_single_row_addr a
    inner join (
        select conv_naspid NASP_ID,CIR_ID,INST_ORD_NUM,CHG_ORD_NUM,MIG_STATUS,DISCO_ORD_NUM,DISCO_DATE_ORDERING_STRT
        from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW
        where report_date < (select max(report_date) from EDW_GLOB_OPS_VW.CIRCUIT_TDM_TD_VW)
        qualify row_number() over(partition by DISCO_ORD_NUM order by REPORT_DATE desc) = 1
    ) b ON a.CIR_ID = b.CIR_ID
    where PROD_YR_MTH = 202502
    and REV_LOC_DIV_CODE in ('LRG','PUB','WHL','SAM')
    and DISCO_DATE_BCOM is null  -- No completion date yet
    and DISCO_DATE_ORDERING_STRT is not null -- Has started disconnection
    and DISCO_ORD_NUM <> ''
    """
    
    # Execute queries
    print("Executing queries...")
    train_data = run_sql(train_query)
    jan_actual_data = run_sql(jan_actual_query)
    jan_predict_data = run_sql(jan_predict_query)
    feb_predict_data = run_sql(feb_predict_query)
    
    print(f"Training data: {len(train_data)} records")
    print(f"January completed disconnects: {len(jan_actual_data)} records")
    print(f"January in-progress disconnects: {len(jan_predict_data)} records")
    print(f"February in-progress disconnects: {len(feb_predict_data)} records")
    
    # Add dummy DISCO_DURATION column for preprocessing
    jan_predict_data['DISCO_DURATION'] = 0
    feb_predict_data['DISCO_DURATION'] = 0
    
    # Initialize data processor
    data_processor = DataProcessor(random_state=42)
    
    # STEP 1: Train initial model with 2023-2024 data
    print("\n=== STEP 1: Training initial model with 2023-2024 data ===\n")
    
    # Process training data
    train_processed = data_processor.preprocess_data(train_data, is_training=True, target_col='DISCO_DURATION')
    
    # Prepare train/test split
    X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled = data_processor.prepare_train_test_data(
        train_processed, target_col='DISCO_DURATION', test_size=0.2
    )
    
    # Initialize ML models
    ml_models = MLModels(random_state=42)
    
    # Train models
    models_to_train = ['rf', 'gb', 'xgb']  # More stable algorithms
    trained_models = ml_models.train_all_models(X_train, y_train, models_to_train=models_to_train)
    
    # Evaluate models
    print("Evaluating models on test data...")
    model_results = ml_models.evaluate_models(X_test, y_test)
    
    # Identify best model
    best_model_name = max(model_results.items(), key=lambda x: x[1]['R2'])[0]
    print(f"Best model is {best_model_name} with R² = {model_results[best_model_name]['R2']:.4f}")
    
    # Save models and preprocessor
    data_processor.save_preprocessor(os.path.join(models_dir, 'data_processor.joblib'))
    ml_models.save_models(models_dir)
    
    # STEP 2: Make predictions for January in-progress disconnects
    print("\n=== STEP 2: Predicting January in-progress disconnect durations ===\n")
    
    # Process January data for prediction
    jan_predict_processed = data_processor.preprocess_data(jan_predict_data, is_training=False, target_col='DISCO_DURATION')
    
    # Ensure feature compatibility
    train_features = set(train_processed.columns)
    jan_predict_features = set(jan_predict_processed.columns)
    
    # Find and fix feature mismatches
    missing_features = train_features - jan_predict_features
    extra_features = jan_predict_features - train_features
    
    print(f"Features missing in January data: {len(missing_features)}")
    print(f"Extra features in January data: {len(extra_features)}")
    
    # Add missing features
    for feature in missing_features:
        if feature != 'DISCO_DURATION':
            jan_predict_processed[feature] = 0
    
    # Remove extra features
    jan_predict_processed = jan_predict_processed.drop(columns=list(extra_features - {'DISCO_DURATION'}), errors='ignore')
    
    # Ensure same column order
    common_columns = list(train_features & jan_predict_features)
    if 'DISCO_DURATION' in common_columns:
        common_columns.remove('DISCO_DURATION')
    jan_predict_processed = jan_predict_processed[common_columns + ['DISCO_DURATION']]
    
    # Make predictions
    X_jan_predict = jan_predict_processed.drop(columns=['DISCO_DURATION'])
    
    # Initialize results DataFrame
    jan_predictions = pd.DataFrame({'CIR_ID': jan_predict_data['CIR_ID']})
    
    # Make predictions with each model
    for name, model in ml_models.best_models.items():
        try:
            preds = model.predict(X_jan_predict)
            jan_predictions[f'prediction_{name}'] = preds
            print(f"Successfully predicted with {name} model")
        except Exception as e:
            print(f"Error predicting with {name} model: {str(e)}")
    
    # Create ensemble prediction
    pred_columns = [col for col in jan_predictions.columns if col.startswith('prediction_')]
    if pred_columns:
        jan_predictions['PREDICTED_DURATION'] = jan_predictions[pred_columns].mean(axis=1)
    
    # Add start date for reference
    jan_predictions['DISCO_DATE_ORDERING_STRT'] = jan_predict_data['DISCO_DATE_ORDERING_STRT']
    
    # Calculate estimated completion date
    if 'PREDICTED_DURATION' in jan_predictions.columns:
        jan_predictions['ESTIMATED_COMPLETION_DATE'] = pd.to_datetime(jan_predictions['DISCO_DATE_ORDERING_STRT']) + \
                                                    pd.to_timedelta(jan_predictions['PREDICTED_DURATION'], unit='D')
    
    # Save January predictions
    jan_output_file = os.path.join(jan_output_dir, 'january_predictions.csv')
    jan_predictions.to_csv(jan_output_file, index=False)
    print(f"January predictions saved to {jan_output_file}")
    
    # STEP 3: Evaluate with January completed disconnects
    print("\n=== STEP 3: Evaluating model with January completed disconnects ===\n")
    
    # Process January actual data
    jan_actual_processed = data_processor.preprocess_data(jan_actual_data, is_training=False, target_col='DISCO_DURATION')
    
    # Ensure feature compatibility
    jan_actual_features = set(jan_actual_processed.columns)
    
    # Find and fix feature mismatches
    missing_features = train_features - jan_actual_features
    extra_features = jan_actual_features - train_features
    
    # Add missing features
    for feature in missing_features:
        if feature != 'DISCO_DURATION':
            jan_actual_processed[feature] = 0
    
    # Remove extra features
    jan_actual_processed = jan_actual_processed.drop(columns=list(extra_features - {'DISCO_DURATION'}), errors='ignore')
    
    # Ensure same column order
    jan_actual_processed = jan_actual_processed[common_columns + ['DISCO_DURATION']]
    
    # Evaluate the model
    X_jan_actual = jan_actual_processed.drop(columns=['DISCO_DURATION'])
    y_jan_actual = jan_actual_processed['DISCO_DURATION']
    
    # Initialize evaluation DataFrame
    jan_evaluation = pd.DataFrame({'CIR_ID': jan_actual_data['CIR_ID'], 'ACTUAL_DURATION': jan_actual_data['DISCO_DURATION']})
    
    # Make predictions with each model
    for name, model in ml_models.best_models.items():
        try:
            preds = model.predict(X_jan_actual)
            jan_evaluation[f'prediction_{name}'] = preds
        except Exception as e:
            print(f"Error evaluating with {name} model: {str(e)}")
    
    # Create ensemble prediction
    pred_columns = [col for col in jan_evaluation.columns if col.startswith('prediction_')]
    if pred_columns:
        jan_evaluation['PREDICTED_DURATION'] = jan_evaluation[pred_columns].mean(axis=1)
        
        # Calculate errors
        jan_evaluation['ERROR'] = jan_evaluation['PREDICTED_DURATION'] - jan_evaluation['ACTUAL_DURATION']
        jan_evaluation['ABS_ERROR'] = np.abs(jan_evaluation['ERROR'])
        jan_evaluation['PCT_ERROR'] = (jan_evaluation['ABS_ERROR'] / jan_evaluation['ACTUAL_DURATION'].replace(0, 1)) * 100
        
        # Calculate metrics
        mae = jan_evaluation['ABS_ERROR'].mean()
        rmse = np.sqrt((jan_evaluation['ERROR'] ** 2).mean())
        r2 = 1 - (np.sum(jan_evaluation['ERROR'] ** 2) / np.sum((jan_evaluation['ACTUAL_DURATION'] - jan_evaluation['ACTUAL_DURATION'].mean()) ** 2))
        
        print(f"January evaluation metrics:")
        print(f"  MAE: {mae:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  R²: {r2:.4f}")
    
    # Save January evaluation
    jan_eval_file = os.path.join(jan_output_dir, 'january_evaluation.csv')
    jan_evaluation.to_csv(jan_eval_file, index=False)
    print(f"January evaluation saved to {jan_eval_file}")
    
    # STEP 4: Enhance model with January feedback
    print("\n=== STEP 4: Enhancing model with January feedback ===\n")
    
    # Combine training data with January completed disconnects
    combined_data = pd.concat([train_data, jan_actual_data], ignore_index=True)
    print(f"Combined training data: {len(combined_data)} records")
    
    # Process combined data
    combined_processed = data_processor.preprocess_data(combined_data, is_training=True, target_col='DISCO_DURATION')
    
    # Prepare train/test split
    X_enhanced, X_test_enhanced, y_enhanced, y_test_enhanced, _, _ = data_processor.prepare_train_test_data(
        combined_processed, target_col='DISCO_DURATION', test_size=0.2
    )
    
    # Retrain models
    enhanced_models = ml_models.train_all_models(X_enhanced, y_enhanced, models_to_train=models_to_train)
    
    # Evaluate enhanced models
    enhanced_results = ml_models.evaluate_models(X_test_enhanced, y_test_enhanced)
    
    # Identify best enhanced model
    best_enhanced_model = max(enhanced_results.items(), key=lambda x: x[1]['R2'])[0]
    print(f"Best enhanced model is {best_enhanced_model} with R² = {enhanced_results[best_enhanced_model]['R2']:.4f}")
    
    # Save enhanced models
    ml_models.save_models(os.path.join(enhanced_output_dir, 'models'))
    data_processor.save_preprocessor(os.path.join(enhanced_output_dir, 'data_processor.joblib'))
    
    # STEP 5: Make predictions for February in-progress disconnects
    print("\n=== STEP 5: Predicting February in-progress disconnect durations ===\n")
    
    # Process February data for prediction
    feb_predict_processed = data_processor.preprocess_data(feb_predict_data, is_training=False, target_col='DISCO_DURATION')
    
    # Ensure feature compatibility
    enhanced_features = set(combined_processed.columns)
    feb_predict_features = set(feb_predict_processed.columns)
    
    # Find and fix feature mismatches
    missing_features = enhanced_features - feb_predict_features
    extra_features = feb_predict_features - enhanced_features
    
    # Add missing features
    for feature in missing_features:
        if feature != 'DISCO_DURATION':
            feb_predict_processed[feature] = 0
    
    # Remove extra features
    feb_predict_processed = feb_predict_processed.drop(columns=list(extra_features - {'DISCO_DURATION'}), errors='ignore')
    
    # Ensure same column order
    common_columns = list(enhanced_features & feb_predict_features)
    if 'DISCO_DURATION' in common_columns:
        common_columns.remove('DISCO_DURATION')
    feb_predict_processed = feb_predict_processed[common_columns + ['DISCO_DURATION']]
    
    # Make predictions
    X_feb_predict = feb_predict_processed.drop(columns=['DISCO_DURATION'])
    
    # Initialize results DataFrame
    feb_predictions = pd.DataFrame({'CIR_ID': feb_predict_data['CIR_ID']})
    
    # Make predictions with each model
    for name, model in ml_models.best_models.items():
        try:
            preds = model.predict(X_feb_predict)
            feb_predictions[f'prediction_{name}'] = preds
            print(f"Successfully predicted with enhanced {name} model")
        except Exception as e:
            print(f"Error predicting with enhanced {name} model: {str(e)}")
    
    # Create ensemble prediction
    pred_columns = [col for col in feb_predictions.columns if col.startswith('prediction_')]
    if pred_columns:
        feb_predictions['PREDICTED_DURATION'] = feb_predictions[pred_columns].mean(axis=1)
    
    # Add start date for reference
    feb_predictions['DISCO_DATE_ORDERING_STRT'] = feb_predict_data['DISCO_DATE_ORDERING_STRT']
    
    # Calculate estimated completion date
    if 'PREDICTED_DURATION' in feb_predictions.columns:
        feb_predictions['ESTIMATED_COMPLETION_DATE'] = pd.to_datetime(feb_predictions['DISCO_DATE_ORDERING_STRT']) + \
                                                    pd.to_timedelta(feb_predictions['PREDICTED_DURATION'], unit='D')
    
    # Save February predictions
    feb_output_file = os.path.join(enhanced_output_dir, 'february_predictions.csv')
    feb_predictions.to_csv(feb_output_file, index=False)
    print(f"February predictions saved to {feb_output_file}")
    
    # STEP 6: Save summary report
    print("\n=== STEP 6: Generating summary report ===\n")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = os.path.join(base_output_dir, f'enhancement_summary_{timestamp}.txt')
    
    with open(summary_file, 'w') as f:
        f.write("=== Circuit Prediction Enhancement Summary ===\n")
        f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("=== Initial Model Performance (2023-2024 Data) ===\n")
        for model_name, metrics in model_results.items():
            f.write(f"\n{model_name}:\n")
            f.write("-" * (len(model_name) + 1) + "\n")
            for metric_name, value in metrics.items():
                f.write(f"{metric_name}: {value:.4f}\n")
        
        f.write("\n\n=== January Evaluation Results ===\n")
        if 'PREDICTED_DURATION' in jan_evaluation.columns:
            f.write(f"Total evaluations: {len(jan_evaluation)}\n")
            f.write(f"MAE: {mae:.4f}\n")
            f.write(f"RMSE: {rmse:.4f}\n")
            f.write(f"R²: {r2:.4f}\n")
        
        f.write("\n\n=== Enhanced Model Performance ===\n")
        for model_name, metrics in enhanced_results.items():
            f.write(f"\n{model_name}:\n")
            f.write("-" * (len(model_name) + 1) + "\n")
            for metric_name, value in metrics.items():
                f.write(f"{metric_name}: {value:.4f}\n")
        
        f.write(f"\n\n=== Prediction Counts ===\n")
        f.write(f"January predictions: {len(jan_predictions)}\n")
        f.write(f"February predictions: {len(feb_predictions)}\n")
        
        f.write(f"\n\nPipeline completed in {time.time() - start_time:.2f} seconds\n")
    
    print(f"Summary report saved to {summary_file}")
    print(f"\nSequential prediction pipeline completed in {time.time() - start_time:.2f} seconds")

if __name__ == '__main__':
    main()



def align_features_for_model(X, model):
    """
    Align features to match exactly what the model expects.
    
    Args:
        X: Input features DataFrame
        model: The trained model with feature_names_in_ attribute
        
    Returns:
        DataFrame with exactly the features the model expects in the right order
    """
    # Get expected feature set from model
    if hasattr(model, 'feature_names_in_'):
        expected_features = model.feature_names_in_
    else:
        # Try to extract from model object in other ways
        print("Model doesn't have feature_names_in_ attribute, prediction may fail")
        return X
    
    # Create new DataFrame with right features
    result = pd.DataFrame(index=X.index)
    
    # First add all expected features
    for feature in expected_features:
        if feature in X.columns:
            result[feature] = X[feature]
        else:
            print(f"Adding missing feature: {feature}")
            result[feature] = 0
            
    # Return aligned features
    return result


# Make predictions with each model
for name, model in ml_models.best_models.items():
    try:
        # Align features to match model expectations exactly
        X_aligned = align_features_for_model(X_jan, model)
        
        # Make prediction
        preds = model.predict(X_aligned)
        jan_predictions[f'prediction_{name}'] = preds
        print(f"Successfully predicted with {name} model")
    except Exception as e:
        print(f"Error predicting with {name} model: {str(e)}")
